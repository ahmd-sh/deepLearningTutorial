{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dl12_cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0O9yQnkPxQWz","colab_type":"code","outputId":"579636f5-9438-46c9-8270-5acc4efb1038","executionInfo":{"status":"ok","timestamp":1544015047641,"user_tz":-330,"elapsed":71981,"user":{"displayName":"Ahmed Shaikh","photoUrl":"https://lh3.googleusercontent.com/-txC82vLcJKA/AAAAAAAAAAI/AAAAAAAAFiY/0j1ju_qzvV4/s64/photo.jpg","userId":"17613041084325769116"}},"colab":{"base_uri":"https://localhost:8080/","height":4413}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n","\n","hm_epochs = 10\n","n_classes = 10\n","batch_size = 128\n","\n","x = tf.placeholder('float', [None, 784])\n","y = tf.placeholder('float')\n","\n","keep_rate = 0.8\n","keep_prob = tf.placeholder(tf.float32)\n","\n","def conv2d(x, W):\n","    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n","\n","def maxpool2d(x):\n","    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","\n","\n","def convolutional_neural_network(x):\n","    weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n","               'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n","               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n","               'out':tf.Variable(tf.random_normal([1024,n_classes]))}\n","\n","    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n","              'b_conv2':tf.Variable(tf.random_normal([64])),\n","              'b_fc':tf.Variable(tf.random_normal([1024])),\n","              'out':tf.Variable(tf.random_normal([n_classes]))}\n","\n","    x = tf.reshape(x, shape=[-1,28,28,1])\n","    \n","    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])\n","    conv1 = maxpool2d(conv1)\n","\n","    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'])\n","    conv2 = maxpool2d(conv2)\n","\n","    fc = tf.reshape(conv2, [-1, 7*7*64])\n","    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n","    \n","    fc = tf.nn.dropout(fc, keep_rate)\n","\n","    output = tf.matmul(fc, weights['out'])+biases['out']\n","\n","    return output\n","\n","def train_neural_network(x):\n","    prediction = convolutional_neural_network(x)\n","    cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)\n","    optimizer = tf.train.AdamOptimizer().minimize(cost)\n","    \n","    hm_epochs = 10\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","\n","        for epoch in range(hm_epochs):\n","            epoch_loss = 0\n","            for _ in range(int(mnist.train.num_examples/batch_size)):\n","                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n","                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n","                epoch_loss += c\n","\n","            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n","\n","        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n","\n","        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n","        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n","\n","train_neural_network(x)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-1-968a6bfda0f7>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.one_hot on tensors.\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From <ipython-input-1-968a6bfda0f7>:52: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","Epoch 0 completed out of 10 loss: [2416507.2 2731502.8 2391109.5 2581645.5 2915456.5 2742306.2 3059017.8\n"," 2814682.8 2359446.2 2203673.  2244511.5 3458328.  2042496.8 2800131.5\n"," 1882794.1 2325664.8 2803198.8 2593827.2 2701925.5 2749125.8 3031747.2\n"," 2125949.  2737440.5 2522412.8 2169283.8 2454688.5 2881162.  2698249.8\n"," 3042714.  2639024.5 2687206.  2344676.2 2268102.5 2629815.8 2632523.8\n"," 2788806.8 3065530.8 2266219.5 2313292.8 2909666.  2141293.8 2563343.8\n"," 2115295.5 2188247.  2488634.  2482449.8 2820143.2 1945230.8 2005755.\n"," 2559685.5 2170857.8 3212377.2 2834703.5 2258056.5 3079477.8 3271337.8\n"," 2775670.5 2855544.2 2970358.5 2558287.  2519064.  2241320.5 2900083.5\n"," 3063690.8 2657559.8 2722086.8 2466672.8 2915852.2 2934653.5 2632706.8\n"," 2307637.5 2729659.  2560696.2 2658938.5 2540446.5 2324075.2 2714702.2\n"," 2456623.2 2494072.8 2519484.2 2688068.  2180627.8 2990813.  2575369.\n"," 2476417.2 2686679.2 2404864.  2745763.5 2382109.  2295796.2 2553188.\n"," 2768014.2 2708160.2 3011759.  2501904.  2802583.  3005644.8 2642847.8\n"," 2732635.2 2931653.  2745046.  2613724.8 2402325.5 2570041.  2621836.2\n"," 2410571.8 2433451.5 2773129.2 2592492.2 2652265.2 2870599.2 2519576.5\n"," 2430929.2 2635065.2 2772056.  2110654.5 3217528.2 2684485.5 2767857.5\n"," 3057261.8 3293079.2 2401024.  2742681.  2662835.2 3113110.2 2431778.5\n"," 2698030.8 3215794.8]\n","Epoch 1 completed out of 10 loss: [466465.97 361350.7  309825.34 439713.66 406161.62 292386.16 429979.22\n"," 310345.47 347760.3  354765.47 433138.28 409881.28 333119.25 483964.84\n"," 478370.12 389548.53 505999.97 268680.03 250295.58 241045.36 390912.66\n"," 393740.28 436030.62 480845.6  300667.62 405306.72 348266.22 220856.8\n"," 280910.12 275467.47 525904.75 274471.94 459182.16 375292.84 400802.2\n"," 416534.88 459121.72 325721.8  331551.16 327162.25 305705.84 399313.8\n"," 361876.44 203219.95 348231.62 342077.84 409529.1  274344.7  487280.25\n"," 298765.97 395056.62 224209.73 372806.4  322685.97 329701.25 294796.9\n"," 436920.2  394942.84 297363.8  402767.38 388550.22 312448.53 475547.75\n"," 265449.1  424014.16 216199.17 356398.75 305905.2  457863.84 290055.16\n"," 382032.44 346556.38 282245.03 366943.28 335106.03 359423.47 445371.9\n"," 552855.94 411659.1  396129.   328530.22 285126.06 517817.47 268728.47\n"," 368139.97 403605.   396349.84 323510.62 303007.78 514245.84 405229.44\n"," 393835.88 408431.   434631.7  364891.62 324590.3  344992.8  474705.47\n"," 417453.72 309375.72 261131.9  433884.8  352157.88 339270.9  398021.03\n"," 409682.7  290739.56 416693.38 322175.72 482350.44 230356.9  410826.97\n"," 368872.56 356208.2  333353.5  398714.56 352257.7  347366.66 414105.03\n"," 239422.2  196225.55 375044.   267193.6  181292.25 249509.78 462163.66\n"," 332413.88 253591.97]\n","Epoch 2 completed out of 10 loss: [168335.72  164412.19  282047.28  199460.45  215737.94  176203.98\n"," 239979.73  280250.47  266130.9   215881.86  157188.33  280332.5\n"," 205475.22  280580.22  248102.55  124125.91  154913.81  237756.69\n"," 139896.45   61832.9   151084.31  241942.64  255578.86  249603.83\n"," 237374.38  278025.22  154184.08  157368.92  155743.9   127158.09\n"," 130200.734 246080.03  137465.22   65639.625 233518.1   131888.11\n"," 126727.22  114702.66  254234.75  205415.56  146944.78  192031.48\n"," 173923.11  150933.52  180819.19  120401.6   170636.4   203412.\n"," 308207.16  141676.75  234884.78  265929.9   207682.69  235967.77\n"," 120332.47  164796.05  168405.7   250998.52  229436.9   176296.02\n"," 137380.84  156177.4    90758.89  175540.61  182944.75  148948.42\n"," 151530.27  149140.42   74456.45  236717.14  102783.08  142861.17\n"," 311523.53  169437.52  220100.06  242430.56  214028.48  149397.83\n"," 108787.516 100109.57  105668.34  220003.08  199748.03  172939.86\n"," 182796.44  141033.31  197041.94  110461.58  233759.    201097.47\n"," 213202.94  146842.14  169695.3   113406.65  161487.33  233118.56\n"," 169671.02  177088.02  208834.64  218123.5   215422.12  127306.664\n"," 212848.08  219477.36  226955.47  178919.23  165999.48  171300.2\n"," 244478.4   139129.48  165755.08  174371.3   137649.69  243207.17\n"," 330129.28  257051.16  262432.75   91916.38  193975.    230673.53\n"," 279045.78  128763.3   155217.47  202331.72  138424.6   214924.58\n"," 224398.86  118638.375]\n","Epoch 3 completed out of 10 loss: [125686.    103069.49   90689.46  116467.52  201602.03   74338.91\n"," 130850.44  141857.58   85787.91  150222.95  186981.67  135109.23\n"," 154321.48  195205.28   88140.05   91570.98  123211.4    94600.84\n"," 101665.96  184945.14  115639.63   67096.85  123080.43   74120.74\n","  92400.8    96664.59  164705.8   180755.06  151624.48  109960.86\n"," 118181.4    62245.96  108641.71  125445.414 104223.04  128515.97\n"," 139161.88  128859.24  134944.02  105937.17  141375.44  131258.6\n","  59801.836 184474.38   76607.75  109030.67   64660.15   69683.36\n"," 124902.3   139933.03  150877.7   185671.9   151470.1   120536.55\n","  66857.805  70122.87   58290.773  63108.88  119425.5    87613.\n"," 144277.88  141889.34  108991.36  158508.23  194720.47   76873.77\n","  89163.42  165118.83   74037.3   194988.2    47311.965 106039.305\n"," 127503.38  113975.47  126767.68   92713.234 155750.88  127059.12\n","  79986.6    89277.69   97156.805 153804.69   95064.93   88088.74\n"," 196765.86   55355.188 139060.     75216.39   66620.74  154516.62\n"," 114036.26  140497.83  126693.34  169487.66  137976.25  137119.12\n","  75234.42   86639.87  117710.414  85218.04  103742.234 143820.22\n"," 114541.89  126227.35  144585.64  119434.51  147946.12   75297.555\n"," 134832.25  104656.234 167328.77   78035.89   94902.984 173560.45\n"," 142991.22   78196.42   80253.04   71383.195 108950.48   64308.484\n"," 110065.4   140260.17  167200.5   105233.695 112977.766 162201.45\n"," 116411.08   56160.297]\n","Epoch 4 completed out of 10 loss: [ 76670.53   32308.836  93091.19   78213.04   60654.527  53381.445\n"," 104018.016 103343.94   84581.664 106331.875  50415.38   66291.875\n"," 139189.25   87285.92   66250.91  118100.19   53644.195  55646.26\n","  96761.27  118718.06   63871.742  83301.56   48613.242  63092.79\n","  79739.234  94366.03   59066.523  65696.54   41572.477 120441.016\n","  93207.984  47761.805 107424.414  52161.727  63671.88   83597.83\n","  52056.836  53899.65   66139.52  133923.88   82718.984  74074.81\n","  27655.2   132122.53   84767.67  133771.3    77258.92   33033.54\n","  90776.83   66367.4    63390.332 107222.94  112470.586  45018.64\n"," 111805.18   79959.99  104145.6   114835.72   77033.86   40881.992\n","  73543.86   40348.195  43574.758  94330.44  104710.016  90977.375\n"," 118649.44   52366.094  60282.1   132799.86   78368.27  105961.7\n","  39840.906 157927.05   33540.234  78692.44  143162.75   50002.77\n","  85480.57   76348.95   51627.773  95522.234  71344.07   46942.78\n"," 126408.45   35681.94  101233.24   39339.42   85186.8    73201.16\n","  47610.99   39882.36   57202.516 108414.67   65580.28  128984.93\n","  93829.47   41628.2   132616.62  109670.445 134319.47   71728.33\n","  88952.38   59363.777  96883.28  130816.234  67044.79   85207.47\n","  84880.27   43239.76   85102.71   86281.69   69960.3    41195.035\n","  63640.664  75773.34  167632.42  124739.55   89325.63   98539.\n","  28187.25  101211.87  102105.88  103545.3    64978.48  140056.14\n","  83317.984  93459.92 ]\n","Epoch 5 completed out of 10 loss: [ 23728.906  34191.6    59046.64   32314.176  50284.656  53736.555\n","  23372.371  62990.71   78859.586  55601.875  24156.19   59867.715\n","  36955.17   39149.656  47841.49   28970.707  37382.855  58265.523\n","  87283.32   58684.75   75121.31   53187.03   81646.4    52428.\n","  46353.14   77658.41   30216.594  41687.344  51515.645  34076.164\n","  93440.055  57321.54   52657.73   44812.688  77221.63   67380.7\n","  41663.668  70295.64   66613.31   59802.445  45980.855  86265.94\n","  36367.312  45964.71   52171.91   59458.367  31607.963  54572.227\n","  86435.4    81611.78   65884.84   58036.508  31917.998  58644.824\n","  98626.88   54509.973  57267.855  36441.938  55809.47   56347.695\n","  54393.336  31333.66   61484.348  82772.35   69533.67   99001.21\n","  38557.156  81339.195  71894.14   73638.93   57959.79   64713.395\n","  55368.63   76371.43   61513.89   62407.55   78520.19   60273.535\n","  86241.81   39653.086  46668.164 152428.55   64611.047  46392.875\n","  60878.36   74661.65   27809.992  45820.7    42313.008  53584.82\n","  58092.297  95551.84   86894.11   73721.516  84529.945  67786.36\n","  80680.76   36970.953  34430.07   99882.625  88554.5    57356.69\n","  48875.633  66898.58   62308.105  53495.973  51295.85   58650.062\n","  76996.92   91131.375  59747.496  74185.78   53625.89   47880.82\n","  85581.82   35100.5    56599.992  22780.133  26908.857  30862.053\n","  45679.67   29910.723  79470.94   58391.496  69354.58   56182.645\n","  78137.27  115090.89 ]\n","Epoch 6 completed out of 10 loss: [ 18508.176  55802.94   11929.099  47380.22  128187.516  89246.484\n","  69426.09   16708.174  85370.99   18510.793  64272.203  51329.383\n","  23417.393  29823.424  32692.373  40519.97   14530.224  49601.188\n","  41843.88   27636.234  73628.92   81449.65   29725.855  58466.117\n","  36983.68   23326.48   50982.363  70477.766  71800.945  42419.625\n","  38902.72   23483.824   9509.477  35468.652  41105.57   62854.09\n","  49312.992  25510.783  39303.24   31162.191  67746.555  57817.055\n","  31277.166  51517.77   43269.832  49754.67   27057.441  30446.406\n","  53240.598   9478.08  104743.67   48602.977  45120.273  50461.266\n","  40094.715  50662.746  56146.805  63425.16   53093.297  26629.031\n","  49409.18   36525.734  22758.285  60322.047  61057.8    67828.61\n","  83876.984  22897.127  24724.691  37244.4    81366.375  28330.21\n","  51936.32   53622.594  72010.67   79148.12   61363.28   52851.957\n","  31119.219  45081.082  17931.547  62774.867  27897.945  45811.227\n","  41782.273  34562.066  21650.805  25615.95   65374.586  33419.453\n","  37661.562  20493.703  16515.049  33681.016  43195.64   35456.22\n","  47177.47   23562.46   69234.984  62072.664  61852.68   32682.389\n","  67060.29   51621.     64342.418  49333.688  42973.438  27880.879\n","  11876.097  26996.797  44103.414  28897.178  32301.559  51742.242\n","  47940.69   24380.672  20014.846  55514.746  55163.992  47857.297\n","  44342.023  23255.648  48378.758  40276.992  51126.746  36826.75\n","  26273.2    38254.797]\n","Epoch 7 completed out of 10 loss: [24725.834  13762.24   20793.406  19638.777   7752.7354 36492.062\n"," 34469.156  40859.492  54862.5    34669.043  21151.45   37193.914\n"," 40163.375  23856.44   17508.146  76200.16   28544.79   28749.777\n"," 59545.477  16091.261  53885.023  11506.455  65234.445  18583.883\n"," 76255.516  54384.008  51274.69   41295.426  32245.418  56124.04\n"," 55450.777  21320.445  16923.758  70453.55   23401.715  37574.297\n"," 26096.176  42324.38   25170.305  17499.398  19660.344  41938.12\n"," 18572.902  23258.812  31162.781  25781.91   75518.03   15148.831\n"," 37813.383  33477.07    7413.3125 68460.77   44299.434  25686.05\n"," 23151.656  29900.496  44710.652  26792.766  24299.14   64130.887\n"," 42381.06   33333.305  28985.914  12809.113  29711.477  23856.86\n"," 54036.195  26225.42   36504.18   33417.15   59623.504  24441.004\n"," 28647.05   32453.205  33957.117  73674.3    27524.787  27446.688\n"," 14379.992  40488.645  37472.22   12582.238  24683.445  45528.258\n"," 68153.195  30553.064  32542.914  20513.629  24413.041  18231.738\n"," 22624.668  31798.65   22945.414  21377.355  53111.68   35321.324\n"," 25275.295  25123.498  82097.625   6586.3926 49462.234  24669.402\n"," 50482.215  32963.36   57550.883  40241.934  34737.348  53246.207\n","  7003.5527 22374.19   53369.223  44959.297  22365.352  27755.654\n"," 10201.758  31163.941  54209.016  52637.15   32395.559  43164.598\n"," 45299.25   35366.7    25694.355  29830.375  18464.459  42463.13\n"," 14579.03   14784.838 ]\n","Epoch 8 completed out of 10 loss: [60568.47   44139.656  25511.395  37244.08    4029.455  37885.906\n","  6057.8555 39342.34   18805.363  41800.258  34749.65   35156.773\n"," 29461.13   14553.809  40460.836  36140.082  16085.458  25194.49\n"," 46514.906  45283.246  20118.07   25597.686  34052.684  19343.941\n","  7889.8027 34565.74   26882.19   11527.545  26188.627  23630.16\n"," 18193.54   32453.86   11323.459  30926.537  16236.76   25680.854\n"," 42651.586  38008.883  17220.562  55852.125  13505.607  25095.031\n"," 40412.7    17582.72   17063.234  33659.047  29813.188  10249.03\n"," 21064.86   18767.627  14775.135  22861.55   10586.081  23199.312\n"," 40306.164  61686.47   43664.516  48348.227  57722.27   45196.883\n"," 20011.205  20997.688  29761.348  25769.238  51707.953  34447.383\n"," 11173.02   27598.113  38220.75   25009.484  15509.49   24045.096\n","  1028.0225 38980.91   18849.42   22411.514  37994.254  30458.62\n","  7484.9854 31217.883  19357.139  24720.629  17198.111   7889.1143\n"," 20345.256   9463.734  21053.637   9275.349  68252.125  20542.984\n"," 23881.422   9472.125  14384.1045 52851.676  26712.613  35666.92\n"," 23743.332  27881.2    34954.418  43074.297  14120.412  65632.59\n"," 50494.51   57076.027  47860.58   37440.414  32627.768  23335.31\n"," 12906.338  11894.994  45266.785  33812.54   22528.338  32599.324\n"," 15536.498  31200.     16050.914  52977.23    7682.636  36484.11\n"," 29705.574  34399.594  48100.92   24074.137  15346.023  33558.15\n"," 16989.785   9233.992 ]\n","Epoch 9 completed out of 10 loss: [22784.727  29350.684  31134.383  11005.471  10285.69   25335.588\n"," 10212.041  36057.38    5640.468  10787.713  36285.87   19113.236\n"," 24086.582  24729.873   6231.9165 17496.344  26604.188  28517.633\n"," 11024.431  17635.31   28677.445  39547.816   9049.543  42159.086\n","  4398.302  12581.     51552.12   10486.021  47378.152  26836.992\n"," 16189.912   9194.424  13814.278  12790.704  43139.75   19118.965\n","  7863.878  11952.45   45955.734  30315.906  35419.29   10597.32\n"," 31350.123  22654.95   14109.65   49538.562  34250.055  30702.086\n"," 15735.884  16664.275  33753.28   21162.73   20225.914  38142.156\n"," 24550.777  24749.566  42730.992  19728.207  23016.709  27124.926\n"," 26677.496  28049.434  22167.209  17766.066  51062.     30712.918\n","  5237.941  26662.799  34744.297   9757.979  17492.459  21147.34\n"," 31332.277  28751.574  33983.805  23689.742  16772.234  15610.247\n"," 15222.509  25467.205  17102.379   2342.752  26038.969  42542.79\n"," 14396.272  40504.64   29906.984  37445.008   7423.787  11321.271\n"," 21238.715  48224.516  29151.71    7611.4844 18183.72    7920.827\n"," 15448.006  10405.271  15953.661  11399.24   33964.918  24264.844\n"," 23690.092  15161.347  22014.143  18232.36   29367.293  26186.756\n"," 13722.286  19677.223  65481.367   8078.6646 46150.875  26104.223\n"," 30073.914  10956.106  22392.734  41670.37   22601.469  28115.992\n"," 23157.473   6890.034  17987.795  21522.646  19946.219  44440.48\n"," 13131.562  19063.617 ]\n","Accuracy: 0.972\n"],"name":"stdout"}]}]}