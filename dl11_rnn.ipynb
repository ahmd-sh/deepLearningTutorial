{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dl9_rnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ea-9DhQ16Bjk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3788},"outputId":"cd5c2bc1-5bfa-4263-8fd2-2f5d66b0c14d","executionInfo":{"status":"ok","timestamp":1543938634470,"user_tz":-330,"elapsed":82144,"user":{"displayName":"Ahmed Shaikh","photoUrl":"https://lh3.googleusercontent.com/-txC82vLcJKA/AAAAAAAAAAI/AAAAAAAAFiY/0j1ju_qzvV4/s64/photo.jpg","userId":"17613041084325769116"}}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","from tensorflow.contrib.rnn.python.ops import rnn, rnn_cell\n","from tensorflow.contrib.rnn import BasicLSTMCell, static_rnn\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n","\n","hm_epochs = 10\n","n_classes = 10\n","batch_size = 128\n","chunk_size = 28\n","n_chunks = 28\n","rnn_size = 128\n","\n","x = tf.placeholder('float', [None, n_chunks,chunk_size])\n","y = tf.placeholder('float')\n","\n","def recurrent_neural_network(x):\n","    layer = {'weights':tf.Variable(tf.random_normal([rnn_size, n_classes])),\n","             'biases':tf.Variable(tf.random_normal([n_classes]))}\n","\n","    x = tf.transpose(x, [1, 0, 2])\n","    x = tf.reshape(x, [-1, chunk_size])\n","    x = tf.split(x, n_chunks, 0)\n","\n","    lstm = BasicLSTMCell(rnn_size, state_is_tuple=True,reuse=True)\n","    (outputs, states) = static_rnn(lstm, x, dtype=tf.float32)\n","\n","    output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n","\n","    return output\n","\n","def train_neural_network(x):\n","    prediction = recurrent_neural_network(x)\n","    cost = tf.nn.softmax_cross_entropy_with_logits( logits=prediction, labels=y)\n","    optimizer = tf.train.AdamOptimizer().minimize(cost)\n","    \n","    \n","    with tf.Session() as sess:\n","        sess.run(tf.initialize_all_variables())\n","\n","        for epoch in range(hm_epochs):\n","            epoch_loss = 0\n","            for _ in range(int(mnist.train.num_examples/batch_size)):\n","                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n","                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n","\n","                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n","                epoch_loss += c\n","\n","            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n","\n","        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n","\n","        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n","        print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))\n","\n","train_neural_network(x)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n","Epoch 0 completed out of 10 loss: [163.30713 183.53133 179.40483 190.69286 176.69325 175.13876 222.83548\n"," 188.98155 183.91153 187.64864 195.34601 182.70192 216.25853 178.71904\n"," 180.30069 194.9237  201.43462 188.50069 152.39937 192.02057 174.64494\n"," 142.86894 159.21674 189.69888 150.15729 173.87595 204.22983 200.74977\n"," 168.84302 211.67165 193.0477  177.14389 182.38614 183.54906 226.03032\n"," 201.18263 175.49786 208.99101 208.24515 171.47362 183.9878  183.80222\n"," 196.1519  156.99098 194.53993 181.66411 197.57687 185.8486  195.5797\n"," 156.36449 170.60425 169.28429 195.30405 201.27342 169.2541  177.87119\n"," 156.94505 188.71115 166.04561 182.15974 170.97328 193.71866 152.1817\n"," 178.92134 200.52786 162.40868 170.14238 164.87366 163.9144  175.54778\n"," 188.277   185.79594 163.54298 177.15367 164.97838 183.82475 183.7958\n"," 192.29536 180.77267 185.75441 203.65582 151.4612  193.34233 212.35512\n"," 215.9578  189.99445 190.5776  210.5662  180.14667 179.24165 190.33344\n"," 201.95541 170.16386 186.37738 192.83142 147.67978 206.91531 219.94774\n"," 182.87392 157.49689 179.24745 202.25162 188.196   191.05133 173.87161\n"," 223.79669 178.00249 189.99686 170.73343 177.9708  189.72015 183.34665\n"," 173.2329  181.89417 182.03276 185.69363 193.46782 169.6211  192.34418\n"," 210.34988 175.97417 185.72102 197.88327 175.35428 179.61545 155.94316\n"," 180.23439 178.69928]\n","Epoch 1 completed out of 10 loss: [55.78209  43.763645 46.88132  54.077618 55.565605 48.78108  41.481785\n"," 53.9629   51.829384 78.72581  46.260864 63.69859  57.994175 60.019604\n"," 69.08982  53.0151   55.20135  57.84433  61.549557 33.290195 75.0814\n"," 61.395004 44.703243 37.66473  58.50259  50.80671  32.977    68.35594\n"," 55.69384  52.11848  55.200153 52.16072  62.15474  64.83887  64.20631\n"," 72.565285 61.914948 53.632713 44.981174 74.57315  45.57743  56.830967\n"," 56.10711  33.60352  59.612595 71.127335 58.68093  53.403206 61.077976\n"," 50.075203 31.516668 65.83706  64.2608   71.917786 72.11146  54.7654\n"," 56.67743  75.54773  71.41955  40.13946  54.305965 44.48758  62.411903\n"," 46.404858 49.60594  60.362576 81.03856  42.508522 48.590805 54.394066\n"," 47.51881  81.9412   77.794945 72.70277  47.042683 39.501743 70.38861\n"," 53.23205  41.26416  84.00897  57.002083 57.395943 45.809216 65.834236\n"," 46.392876 49.01278  37.493755 64.65686  59.144814 45.851933 68.782684\n"," 58.3422   48.617077 65.59213  64.15236  51.09488  68.78565  53.30902\n"," 74.04539  61.02967  74.39438  59.081097 60.46781  41.29316  44.091175\n"," 73.58881  45.054794 51.53538  51.835537 34.304913 59.9485   49.01342\n"," 65.929726 64.09371  58.097656 62.77781  43.26558  45.415234 68.86472\n"," 55.159714 47.70891  63.03575  55.937103 67.156075 57.43144  58.953754\n"," 59.015957 50.08853 ]\n","Epoch 2 completed out of 10 loss: [34.751514 27.685522 66.49506  59.2658   26.358368 44.19589  42.165977\n"," 30.702057 32.86323  32.893013 56.017212 56.101818 24.246862 37.907745\n"," 77.65941  32.037605 29.798609 31.573948 33.61604  49.33496  57.55762\n"," 51.2018   35.208675 46.47675  34.882748 34.227146 57.01318  44.615067\n"," 37.589012 75.00909  52.49462  32.972816 50.111816 28.773842 28.884521\n"," 23.65956  54.028587 34.97228  20.516155 31.293827 34.552547 65.853264\n"," 33.317677 47.94701  34.50474  37.41826  35.950207 24.65329  27.271376\n"," 51.25922  28.40759  39.597763 46.154987 46.340496 28.359571 37.07505\n"," 21.15556  45.773663 47.448803 32.288445 35.12596  42.53707  24.724943\n"," 27.791529 42.266994 44.822422 40.274685 26.548347 42.48196  29.083542\n"," 42.903717 53.063553 34.270184 41.768208 44.26961  34.774616 49.61648\n"," 21.806606 38.480625 28.953247 26.359516 49.732723 41.73712  53.748672\n"," 39.846375 29.973955 34.429302 24.232729 40.787254 46.008663 36.286762\n"," 39.59821  37.32697  57.22497  38.198807 30.10214  53.20814  25.715744\n"," 47.107483 47.44246  28.332195 42.068634 24.678267 54.418713 52.843845\n"," 23.876657 47.77985  50.165836 33.02733  33.671658 43.95978  54.525993\n"," 32.67796  31.852633 37.18196  22.728632 43.839916 50.933796 32.30579\n"," 59.161034 38.844654 23.50304  28.318888 34.659843 53.679256 41.212406\n"," 43.599792 21.349045]\n","Epoch 3 completed out of 10 loss: [23.740395 26.04945  20.83012  22.221714 21.575548 38.744877 49.915558\n"," 19.689438 39.919228 24.130762 30.909033 22.168966 28.407253 23.783928\n"," 35.592537 34.91376  30.712057 35.70593  45.59626  23.002634 41.216793\n"," 25.81697  28.044607 23.86156  50.071472 19.956165 43.123684 33.56034\n"," 27.274336 31.049122 53.063614 30.313503 42.88173  24.409588 28.161207\n"," 49.29499  24.025507 44.794098 53.07492  49.94676  35.688    42.37864\n"," 31.269632 29.828537 17.013136 26.874695 30.966703 38.569916 30.995022\n"," 33.472492 26.70205  28.230272 24.181906 24.059832 34.473442 21.888103\n"," 26.618603 19.625528 30.331848 23.136347 35.667202 32.3405   26.952444\n","  8.44281  28.03391  31.39394  38.05006  31.317333 23.34891  29.764725\n"," 24.82021  15.910705 31.056969 44.675022 18.251217 35.00275  28.259346\n"," 18.58268  41.834614 23.083797 28.120228 41.573677 16.346113 28.950848\n"," 31.681452 27.255133 21.08624  29.39316  31.132402 12.1426   30.115507\n"," 10.543814 24.782074 32.222115 30.915615 30.772055 33.045574 18.658274\n"," 18.23917  20.5495   48.82016  31.335215 30.928867 26.009909 32.90717\n"," 28.350054 33.02305  18.99633  29.25754  28.340712 43.095005 17.283817\n"," 28.8917   25.433578 40.898453 31.352272 27.324587 42.21449  47.596783\n"," 37.413612 27.082567 21.1318   32.50676  20.199224 39.89874  22.99886\n"," 22.861725 43.142628]\n","Epoch 4 completed out of 10 loss: [19.943104  27.724783  19.426907  15.556565  14.326287  15.628989\n"," 27.298817  17.728369  20.020525  17.584309  15.075745  28.839317\n"," 17.128492  37.244095  39.33538   25.263538  21.88723   29.65086\n"," 28.094595  17.290758  17.674873  12.038484  30.100763  30.11089\n"," 17.066229  25.434122  13.473744  33.268066  23.015062  24.125914\n"," 18.308666  24.641567  28.928959  23.763487  29.419178  29.781084\n"," 25.969305  33.97181   18.450506  16.343904  29.641052  15.730585\n"," 10.0659895 12.755223  11.708135  41.65378   33.38907   28.384123\n"," 24.255268  21.582388  18.944778  17.479881  21.198551  19.273216\n"," 20.058823  18.32455   15.865771  14.730643  23.542559  26.693626\n"," 22.389452  25.431126  53.1126    28.328987  20.19275   49.51054\n"," 16.251766  18.668392  26.320555  20.580511  12.712319  16.582735\n"," 16.902206  40.542324  26.371338  19.554567  30.753923  30.459126\n"," 30.158693  33.279446  32.52866   33.98291   18.618958  32.6576\n"," 25.308392  33.234276  16.806873  18.41937   25.644545  23.568548\n"," 20.549284  23.74073   34.12115   15.175413  22.877739  24.141058\n"," 27.859148  36.762135  34.136562  28.23989   15.53686   13.867951\n"," 32.25855   31.497784  25.56359   18.357456  17.467894  23.911789\n"," 32.71963   36.5711    14.624954  24.500889  22.741632  11.074782\n"," 30.498672  32.0584    17.848387  28.029919  21.99014   34.765976\n"," 26.437162  23.722488  11.847735  14.536224  33.4857    34.682323\n"," 26.272194  26.441265 ]\n","Epoch 5 completed out of 10 loss: [11.899196  28.031187  19.233831  23.510176  19.80025   13.63607\n"," 15.249727  24.824299  13.141763  40.619427  19.735182  11.014823\n"," 10.021474  20.625622  18.409594  26.320608   6.7206373 11.157247\n"," 31.782959  20.28956   24.270514  13.337726  15.094906   6.492179\n"," 19.588676  24.048061  19.756083  22.618898  20.747978  24.65934\n"," 24.982862  15.086252  14.034891  19.598831  17.370026  25.36808\n"," 24.273148  21.29158   13.035514  19.85644   24.433365  27.985907\n"," 17.087767  15.415765  22.403015  27.39123    9.011843  21.753475\n"," 17.12782   29.81688   23.835493  24.560493  18.890951  12.496712\n"," 24.039772  11.31399   16.71983   19.543173  14.594145  33.109447\n"," 28.16469    6.717579   9.818383  20.706196  44.632122  22.115667\n"," 18.252554  12.326619  26.421608  11.16486   18.758184  35.405243\n"," 16.47674   12.439338   9.46431   25.072834  11.143896   5.7041664\n"," 17.441912   7.6810474 19.969767  21.409683  20.571432  19.158869\n"," 18.27121   17.594143  32.42823   16.609793  22.64768   29.24734\n","  6.4785037 13.8440275 32.07618   26.257471  19.903559  16.216574\n"," 25.159763  11.493306  15.800453  17.762104  20.300571  14.996932\n"," 27.121279  12.851321  21.514647  17.127064  30.14705   29.277012\n"," 22.396208  15.654635  21.793402  19.005226  22.04917   17.425695\n"," 38.59969   22.537214  13.738144  18.068771  18.659155  16.995874\n"," 19.753563  14.461599  20.016525  17.121937  29.921408  17.077696\n"," 31.396034  14.66022  ]\n","Epoch 6 completed out of 10 loss: [23.289104  20.35065   13.75018   14.933341  10.349422  13.480193\n"," 26.716688  16.282143  17.302807  18.983934  10.78499   15.784847\n"," 52.875957   9.374216  16.887354  24.939255  18.120085  19.937733\n"," 27.497965  13.912016   7.004534   6.313163  13.28844   26.276848\n","  7.261223  22.239195  10.72083   16.88153   33.22744   12.523183\n"," 13.870157  29.430567  11.65207   23.399822  12.743168  10.629703\n"," 12.367855  29.749527  25.584002  22.622162  31.184526  10.955973\n"," 24.03987   21.323399  11.8778305 15.844204  13.507811   9.551706\n"," 10.128348   7.5213146 12.631896  25.478727   9.4380045  7.4105005\n"," 23.661514  25.410923  20.60581   17.24262    9.776172  15.213988\n"," 21.434504  31.848343  19.529072  13.972804  21.970436  18.30933\n"," 11.195198  10.924503  20.302744  27.08897   15.726655  31.6335\n"," 27.049017  18.470602  11.768048  17.7069    13.440748  11.958006\n"," 15.516633  19.804083  23.171722   8.234341  22.43296   17.571692\n"," 15.325075  12.223879  22.638527   7.3365407 14.79997   15.845156\n"," 16.62481   22.721266  15.040797  17.73194   15.173751  16.690622\n"," 10.614853  28.06051   28.790638  17.034723  15.550996  18.471918\n"," 16.219221  20.64796   20.437536  17.505081  16.858852  12.630572\n"," 17.62126    6.358452  24.94218   20.125725  26.622087  11.935398\n"," 13.609554  17.958199  12.0315075 21.758444  14.820062  11.070556\n"," 27.566378  23.26934   22.239763  16.105589  10.201304  20.289198\n","  9.62197   19.096878 ]\n","Epoch 7 completed out of 10 loss: [ 9.485093  16.796148  10.379442  15.457109  13.81066   14.756565\n"," 11.347951  21.645329  13.52817   13.72255   13.110949   9.121591\n"," 16.351198   5.300183  11.581953  14.795248  10.215176  31.137115\n"," 22.023327  16.328382  12.687673  19.444834  10.273131  16.969982\n"," 11.903264  31.245676  10.291584  25.090195  28.428133   9.434116\n"," 23.404663  11.349777  19.904531   6.4265585  8.137123  27.125832\n"," 18.5626     9.531434  14.132939  17.632507   2.8640802 21.993635\n"," 10.503965  11.797343  16.26425   23.407394  12.047607  19.40489\n","  9.268777  18.017096  16.034458  16.121614  16.249563  16.26755\n","  8.318383  22.331366  15.217743  12.999459  19.719385  13.964872\n"," 12.932066  29.624935  22.75371   11.595562   7.3409424 22.719547\n"," 14.239131  18.600967  14.755713   8.617963  10.611856   8.883865\n"," 22.788294  14.111726  18.690395  21.740421  12.603911  10.699928\n"," 13.321021  10.658889   9.727734  12.3972845 10.658211   8.602679\n"," 13.320844   7.5262218 11.9304085 30.525578  19.007698  21.517519\n"," 17.528917  14.61896   21.451849  12.258578   9.828532  17.994312\n","  7.679742  24.383518  18.621979  14.183197  17.172894  32.185287\n"," 15.342026  16.419647  16.264767  13.64382   20.23047   14.467555\n"," 18.107475  11.091699  20.13465   10.400074  17.448397  10.347157\n"," 20.297459  16.8194    10.492653  18.866533  19.294014   7.594048\n","  6.9436173 12.040572  13.8699665 24.56523    5.0508895 16.12865\n"," 25.169172  16.55509  ]\n","Epoch 8 completed out of 10 loss: [ 6.9251356 32.218586   5.92979    3.9181032 11.162355  20.05411\n","  7.9027185  7.217737  15.707336  15.485235  19.159494  18.15592\n"," 10.137897  21.030014   6.722851  15.52512   30.422888   9.393011\n","  3.3399677  5.9463215  7.374844  21.069271  10.7334385  8.46003\n","  3.0093899 18.678404  17.326363   6.2357244 17.51334   10.935472\n"," 10.330754   4.96087   18.242973  14.997443   7.534286  17.8759\n"," 11.875133  14.8892    22.16435    5.0293903 32.87418   11.026241\n","  7.9351654 15.98335   30.251503  17.291458  12.152541  12.093772\n","  6.707681   4.5522165  8.60854   19.957815  18.310265  14.652522\n"," 19.040531  10.319501  10.88504   14.1951885  8.671256   5.584303\n"," 16.241463  12.502993  15.515715  15.57961   12.748073  10.294647\n","  6.562358  14.028788  19.99953   13.4179945 11.508763  25.66592\n"," 10.309363   7.2951074  7.4935427  7.9701085 13.467814  16.888659\n"," 13.326289   8.029143   4.8859076 23.523352  30.50829   14.921126\n"," 19.338745   9.73162   23.48472   14.333093  15.322566  11.798217\n"," 14.267027   8.545075  15.351798  37.41525    4.023797  13.59288\n"," 13.659673  19.413845  10.365878   9.89546    8.42354   11.532193\n","  8.199419  16.447315   7.7735195 10.415243  11.002197   4.727426\n"," 11.971443  10.750155   6.4524765 19.972754  16.379803  17.607464\n","  9.520348   5.5009995  5.1897864 10.587501   9.571039  24.32415\n"," 15.37599    6.0718083 16.570436   9.761108  24.548553   9.221829\n"," 29.204622  22.795446 ]\n","Epoch 9 completed out of 10 loss: [12.093774  12.782996  19.77739   13.5949     9.668266  14.485234\n"," 11.932128  10.653212   6.5071983 17.057755  13.224998  17.767447\n"," 15.367258   9.57424    5.601418   4.834356  13.569846  24.993162\n"," 27.008135   6.9111195 11.549306   9.564706  13.418619   4.407135\n","  5.967773   5.389161   6.3925877  5.510139  13.943198  17.807573\n"," 11.980613   8.362763  18.219091  21.277308   7.839241  13.520884\n"," 17.455194   9.515358   4.932931  11.707405  18.954762  15.520941\n","  6.8482423  5.352502   7.877319  14.720672  11.967381  14.835742\n","  6.7703624 11.872959   7.723259  12.456634   4.227222  16.508703\n","  6.153457   2.8625674  7.7914286  7.4678135 12.127232   4.497287\n","  7.329314  17.24405   15.799318   7.046652  11.455262   8.162145\n"," 25.152988  20.956606  15.798766  13.523142  16.031462   5.1062617\n","  9.284267  10.227103  21.624838  16.8041    13.682877  17.495743\n","  7.6819005 14.217911  24.65372   30.086653   6.170751  10.368845\n"," 12.321674   4.355976   8.181946  14.951724  16.775898  33.303062\n"," 15.492262  14.563188  17.130106  16.232697   9.1507845 13.120392\n","  5.8726597  7.028404  10.15314   11.27305   15.0410595  7.1839094\n"," 11.289575  10.663704  13.30427    8.179926  10.253128  17.30685\n"," 14.757167  12.751014  16.8538     9.45896    8.308674  13.09623\n"," 13.033241   7.1241155 25.592106   8.528235   7.31519    8.332086\n"," 10.137632  15.454006   7.606075  12.197394   8.4869375 11.431918\n"," 15.355737  14.447276 ]\n","Accuracy: 0.9841\n"],"name":"stdout"}]}]}